{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1uyT8pUj3yxCxQen0nX2HI_B2U279ceA3","authorship_tag":"ABX9TyOLqUsDL+V4XJb6AqzU/Mol"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PM--0PV5ILjb"},"source":["# 모듈 선언\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import random\n","import cv2, dlib\n","import time\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from imutils import face_utils\n","from google.colab.patches import cv2_imshow\n","from tqdm import tqdm\n","\n","# Parameters\n","TH1 = 0.5  # Confidence\n","TH2 = 0.25 # Non-maximum Suppression\n","\n","# CNN model\n","config = '/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/yolov3.cfg' \n","model = '/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/yolov3.weights' \n","classLabels = '/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/coco.names'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYoTu7_GIu5S","executionInfo":{"status":"ok","timestamp":1633232663793,"user_tz":-540,"elapsed":3,"user":{"displayName":"김준용","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhkG7V_8ZEeODMWUbjWe9IxOi5v12Nk4hpgorT_zQ=s64","userId":"07738805178596517829"}},"outputId":"dc081181-7714-4bca-920f-26c3eac2f312"},"source":["x_train = np.load('/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/dataset/x_train.npy').astype(np.float32)\n","y_train = np.load('/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/dataset/y_train.npy').astype(np.float32)\n","x_val = np.load('/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/dataset/x_val.npy').astype(np.float32)\n","y_val = np.load('/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/dataset/y_val.npy').astype(np.float32)\n","\n","print(x_train.shape, y_train.shape)\n","print(x_val.shape, y_val.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2586, 26, 34, 1) (2586, 1)\n","(288, 26, 34, 1) (288, 1)\n"]}]},{"cell_type":"code","metadata":{"id":"isnfPaWlIu79"},"source":["# Image Augmentation(옵션을 더 많이 줘서 개선)\n","train_argmentation = ImageDataGenerator(\n","    rescale=1./255, # 이미지 0-1 사이의 값으로 변경\n","    rotation_range=10,\n","    zoom_range=0.2,\n","    shear_range=0.2,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    vertical_flip=False\n",")\n","\n","val_argmentation = ImageDataGenerator(\n","    rescale=1./255\n",")\n","\n","augment_size = 50\n","train_generator = train_argmentation.flow(\n","    x=x_train, y=y_train,\n","    batch_size=augment_size,\n","    shuffle=True\n",")\n","val_generator = val_argmentation.flow(\n","    x=x_val, y=y_val,\n","    batch_size=augment_size,\n","    shuffle=False\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IKhOdSJVIu-a","executionInfo":{"status":"ok","timestamp":1633232664082,"user_tz":-540,"elapsed":3,"user":{"displayName":"김준용","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhkG7V_8ZEeODMWUbjWe9IxOi5v12Nk4hpgorT_zQ=s64","userId":"07738805178596517829"}},"outputId":"5d543073-4e04-49ad-dfdb-c03801088159"},"source":["# CNN 모델 생성(tensorflow로 개선)\n","cnn_model = tf.keras.Sequential([\n","    tf.keras.layers.Conv2D(input_shape=(26, 34, 1), kernel_size=(3,3), filters=32, strides=1, padding='same', activation='relu'),\n","    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n","\n","    tf.keras.layers.Conv2D(kernel_size=(3,3), filters=64, padding='same', strides=1, activation='relu'),\n","    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n","\n","    tf.keras.layers.Conv2D(kernel_size=(3,3), filters=128, padding='same', strides=1, activation='relu'),\n","    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n","\n","    tf.keras.layers.Flatten(),\n","\n","    tf.keras.layers.Dense(units=512, activation='relu'),\n","    tf.keras.layers.Dense(units=256, activation='relu'),\n","    tf.keras.layers.Dense(units=1, activation='sigmoid')\n","])\n","\n","cnn_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n","cnn_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_3 (Conv2D)            (None, 26, 34, 32)        320       \n","_________________________________________________________________\n","max_pooling2d_3 (MaxPooling2 (None, 13, 17, 32)        0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 13, 17, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 6, 8, 64)          0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 6, 8, 128)         73856     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 3, 4, 128)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 1536)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 512)               786944    \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 256)               131328    \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 1,011,201\n","Trainable params: 1,011,201\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vu4ZhhYYIzqf","executionInfo":{"status":"ok","timestamp":1633232707066,"user_tz":-540,"elapsed":42986,"user":{"displayName":"김준용","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhkG7V_8ZEeODMWUbjWe9IxOi5v12Nk4hpgorT_zQ=s64","userId":"07738805178596517829"}},"outputId":"049a4faf-5f82-4803-cba6-bc59337d3719"},"source":["# 모델 학습\n","history = cnn_model.fit_generator(train_generator, epochs=25, validation_data=val_generator)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/25\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"]},{"output_type":"stream","name":"stdout","text":["52/52 [==============================] - 3s 36ms/step - loss: 0.4850 - accuracy: 0.7664 - val_loss: 0.2421 - val_accuracy: 0.9097\n","Epoch 2/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.2836 - accuracy: 0.8844 - val_loss: 0.1303 - val_accuracy: 0.9583\n","Epoch 3/25\n","52/52 [==============================] - 2s 33ms/step - loss: 0.1822 - accuracy: 0.9308 - val_loss: 0.0776 - val_accuracy: 0.9757\n","Epoch 4/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.1680 - accuracy: 0.9370 - val_loss: 0.0643 - val_accuracy: 0.9757\n","Epoch 5/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.1500 - accuracy: 0.9482 - val_loss: 0.0834 - val_accuracy: 0.9653\n","Epoch 6/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.1192 - accuracy: 0.9571 - val_loss: 0.0591 - val_accuracy: 0.9757\n","Epoch 7/25\n","52/52 [==============================] - 2s 31ms/step - loss: 0.1043 - accuracy: 0.9675 - val_loss: 0.0488 - val_accuracy: 0.9826\n","Epoch 8/25\n","52/52 [==============================] - 2s 31ms/step - loss: 0.1453 - accuracy: 0.9509 - val_loss: 0.0551 - val_accuracy: 0.9792\n","Epoch 9/25\n","52/52 [==============================] - 2s 31ms/step - loss: 0.0943 - accuracy: 0.9660 - val_loss: 0.0536 - val_accuracy: 0.9826\n","Epoch 10/25\n","52/52 [==============================] - 2s 31ms/step - loss: 0.0944 - accuracy: 0.9667 - val_loss: 0.0605 - val_accuracy: 0.9757\n","Epoch 11/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0800 - accuracy: 0.9756 - val_loss: 0.0605 - val_accuracy: 0.9792\n","Epoch 12/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0679 - accuracy: 0.9780 - val_loss: 0.0362 - val_accuracy: 0.9861\n","Epoch 13/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0625 - accuracy: 0.9799 - val_loss: 0.0530 - val_accuracy: 0.9826\n","Epoch 14/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0777 - accuracy: 0.9756 - val_loss: 0.0722 - val_accuracy: 0.9688\n","Epoch 15/25\n","52/52 [==============================] - 2s 31ms/step - loss: 0.0668 - accuracy: 0.9780 - val_loss: 0.0541 - val_accuracy: 0.9792\n","Epoch 16/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0575 - accuracy: 0.9814 - val_loss: 0.0519 - val_accuracy: 0.9792\n","Epoch 17/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0571 - accuracy: 0.9818 - val_loss: 0.0668 - val_accuracy: 0.9826\n","Epoch 18/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0670 - accuracy: 0.9768 - val_loss: 0.0542 - val_accuracy: 0.9792\n","Epoch 19/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0550 - accuracy: 0.9826 - val_loss: 0.0409 - val_accuracy: 0.9896\n","Epoch 20/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0501 - accuracy: 0.9865 - val_loss: 0.0404 - val_accuracy: 0.9826\n","Epoch 21/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0384 - accuracy: 0.9892 - val_loss: 0.0622 - val_accuracy: 0.9826\n","Epoch 22/25\n","52/52 [==============================] - 2s 31ms/step - loss: 0.0388 - accuracy: 0.9888 - val_loss: 0.0828 - val_accuracy: 0.9792\n","Epoch 23/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0415 - accuracy: 0.9845 - val_loss: 0.0460 - val_accuracy: 0.9861\n","Epoch 24/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0410 - accuracy: 0.9888 - val_loss: 0.0553 - val_accuracy: 0.9896\n","Epoch 25/25\n","52/52 [==============================] - 2s 32ms/step - loss: 0.0457 - accuracy: 0.9849 - val_loss: 0.0390 - val_accuracy: 0.9861\n"]}]},{"cell_type":"code","metadata":{"id":"5QLVYZIuJE7_"},"source":["# https://github.com/kairess/eye_blink_detector/blob/master/test.py\n","IMG_SIZE = (34, 26)\n","\n","# 얼굴 특징점 추출\n","detector = dlib.get_frontal_face_detector()  # face detector\n","predictor = dlib.shape_predictor('/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/shape_predictor_68_face_landmarks.dat')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iCOvzlHVJDgw"},"source":["def crop_eye(img, eye_points):\n","    x1, y1 = np.amin(eye_points, axis=0)\n","    x2, y2 = np.amax(eye_points, axis=0)\n","    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n","\n","    w = (x2 - x1) * 1.2\n","    h = w * IMG_SIZE[1] / IMG_SIZE[0]\n","\n","    margin_x, margin_y = w / 2, h / 2\n","\n","    min_x, min_y = int(cx - margin_x), int(cy - margin_y)\n","    max_x, max_y = int(cx + margin_x), int(cy + margin_y)\n","\n","    eye_rect = np.rint([min_x, min_y, max_x, max_y]).astype(np.int)  # 정수 array로 변환\n","\n","    eye_img = gray[eye_rect[1]:eye_rect[3], eye_rect[0]:eye_rect[2]]\n","\n","    return eye_img, eye_rect"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"41s6HR_qIW8S"},"source":["# from darknet.py\n","# yolo\n","def fun_get_colors(names):\n","    \"\"\"\n","    Create a dict with one random BGR color for each\n","    class name\n","    \"\"\"\n","    random.seed(1)\n","    return {name: (\n","        random.randint(0, 255),\n","        random.randint(0, 255),\n","        random.randint(0, 255)) for name in names}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8kK10XdZIW_A","executionInfo":{"status":"ok","timestamp":1633233436040,"user_tz":-540,"elapsed":727362,"user":{"displayName":"김준용","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhkG7V_8ZEeODMWUbjWe9IxOi5v12Nk4hpgorT_zQ=s64","userId":"07738805178596517829"}},"outputId":"401413af-e6e1-461a-cdab-d87311b3758e"},"source":["# main(동영상 저장과 경고 알림 추가)\n","if __name__ == \"__main__\":\n","    # Load CNN model trained over MS COCO DB\n","    net = cv2.dnn.readNetFromDarknet(config, model)\n","    layerNames = net.getLayerNames()\n","    layerOutputs = [layerNames[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n","    with open(classLabels, 'rt') as file:\n","        classNames = file.read().strip('\\n').split('\\n')\n","    classColors = fun_get_colors(classNames)\n","    \n","    cap = cv2.VideoCapture('/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/videos/use_cellphone3.mp4')\n","    total_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    cnt = 0\n","\n","    for now_frame in tqdm(range(total_frame)):\n","        ret, img_ori = cap.read()\n","\n","        if not ret:\n","            break\n","\n","        img_ori = cv2.resize(img_ori, dsize=(0, 0), fx=0.5, fy=0.5)\n","\n","        img = img_ori.copy()\n","        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","        # Run CNN: input image to output tensor\n","        try:\n","            blob = cv2.dnn.blobFromImage(img, 1/255, (416, 416), swapRB=1)\n","            # blob = cv2.dnn.blobFromImage(img, 1/255, (320, 320), swapRB=1)\n","            net.setInput(blob)\n","            outs = net.forward(layerOutputs)\n","        except Exception as e:\n","            print(str(e))\n","\n","        faces = detector(gray)\n","\n","        # Non-maximum Suppression\n","        imgH, imgW, _ = img.shape\n","        classIDs = []\n","        confidences = []\n","        boxes = []\n","        for out in outs:\n","            for detection in out:\n","                scores = list(detection[5:])\n","                confidence = max(scores)\n","                classID = scores.index(confidence)\n","                if confidence > TH1:\n","                    centerX, centerY   = int(detection[0] * imgW), int(detection[1] * imgH)\n","                    boxW, boxH         = int(detection[2] * imgW), int(detection[3] * imgH)\n","                    topleftX, topleftY = int(centerX - boxW / 2), int(centerY - boxH / 2)\n","                    boxes.append([topleftX, topleftY, boxW, boxH])\n","                    confidences.append(float(confidence))\n","                    classIDs.append(int(classID))\n","        resultDet = cv2.dnn.NMSBoxes(boxes, confidences, TH1, TH2)\n","\n","        # Visualization\n","        for ind in resultDet:\n","            id = ind[0]\n","            topleftX, topleftY, boxW, boxH = boxes[id]\n","            label = f'{classNames[classIDs[id]]}: {confidences[id]:.2}'\n","            if \"cell phone\" in label:\n","                cv2.putText(img, \"Warning!\", (100, 300), cv2.FONT_HERSHEY_PLAIN, 12.0, (0, 0, 255), 5); # 입력 이미지, 입력 문구, 문구 시작 위치, 폰트, 글자크기, 글자색상, 글자굵기\n","            color = classColors[classNames[classIDs[id]]]\n","            cv2.rectangle(img, (topleftX, topleftY, boxW, boxH), color, 2)\n","            cv2.putText(img, label, (topleftX, topleftY-12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n","        eTime, _ = net.getPerfProfile()\n","        # print(\"Elapsed time: %.2f ms\" % (eTime * 1000.0 / cv.getTickFrequency()))\n","        \n","        for face in faces:\n","            shapes = predictor(gray, face)\n","            shapes = face_utils.shape_to_np(shapes)\n","\n","            eye_img_l, eye_rect_l = crop_eye(gray, eye_points=shapes[36:42])\n","            eye_img_r, eye_rect_r = crop_eye(gray, eye_points=shapes[42:48])\n","\n","            eye_img_l = cv2.resize(eye_img_l, dsize=IMG_SIZE)\n","            eye_img_r = cv2.resize(eye_img_r, dsize=IMG_SIZE)\n","            eye_img_r = cv2.flip(eye_img_r, flipCode=1)\n","\n","            eye_input_l = eye_img_l.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n","            eye_input_r = eye_img_r.copy().reshape((1, IMG_SIZE[1], IMG_SIZE[0], 1)).astype(np.float32) / 255.\n","\n","            pred_l = cnn_model.predict(eye_input_l)\n","            pred_r = cnn_model.predict(eye_input_r)\n","\n","            # visualize\n","            state_l = '%.1f' if pred_l > 0.1 else '%.1f'\n","            state_r = '%.1f' if pred_r > 0.1 else '%.1f'\n","\n","            state_l = state_l % pred_l\n","            state_r = state_r % pred_r\n","\n","            if state_l == '0.0' and state_r == '0.0':\n","                cnt += 1\n","            if cnt > 30:\n","                cv2.putText(img, \"Warning!\", (100, 300), cv2.FONT_HERSHEY_PLAIN, 12.0, (0, 0, 255), 5)\n","            if state_l > '0.3' or state_r > '0.3':\n","                cnt = 0\n","\n","            cv2.rectangle(img, pt1=tuple(eye_rect_l[0:2]), pt2=tuple(eye_rect_l[2:4]), color=(255,255,255), thickness=2)\n","            cv2.rectangle(img, pt1=tuple(eye_rect_r[0:2]), pt2=tuple(eye_rect_r[2:4]), color=(255,255,255), thickness=2)\n","\n","            cv2.putText(img, state_l, tuple(eye_rect_l[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n","            cv2.putText(img, state_r, tuple(eye_rect_r[0:2]), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,255), 2)\n","\n","        cv2.imwrite(f\"/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/result/{now_frame:.0f}.jpg\", img)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 320/320 [12:01<00:00,  2.25s/it]\n"]}]},{"cell_type":"code","metadata":{"id":"wc4hi-xLIXBT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633233441355,"user_tz":-540,"elapsed":5319,"user":{"displayName":"김준용","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhkG7V_8ZEeODMWUbjWe9IxOi5v12Nk4hpgorT_zQ=s64","userId":"07738805178596517829"}},"outputId":"38b736d0-20e3-478a-90b1-3658122dc22f"},"source":["# picture to video\n","# 참고 : https://hanryang1125.tistory.com/19\n","import glob\n","\n","img_array = []\n","\n","for file in tqdm(glob.glob('/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/result/*.jpg')):\n","    img = cv2.imread(file)\n","    img_array.append(img)\n","\n","height, width = img.shape[:2]\n","size = (width, height)\n","video = cv2.VideoWriter('/content/drive/MyDrive/Colab Notebooks/ComputerVision/project/result/result.avi',cv2.VideoWriter_fourcc(*'DIVX'), 20, size)\n","\n","for img in img_array:\n","    video.write(img)\n","\n","video.release()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 320/320 [00:04<00:00, 78.32it/s]\n"]}]}]}